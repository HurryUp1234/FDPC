Highlights
?
A novel dynamic density peaks clustering method called DPC-DBFN is proposed.

?
A fuzzy kernel is proposed to compute the local densities of the data points.

?
A graph-based label propagation strategy is used to identify backbones, border areas and noisy points.

?
DPC-DBFN can effectively assign true labels to border points located in overlapped regions.

?
The results on real-world, images and synthetic data show the effectiveness of the proposed method.


Abstract
Density peaks clustering (DPC) is as an efficient clustering algorithm due for using a non-iterative process. However, DPC and most of its improvements suffer from the following shortcomings: (1) highly sensitive to its cutoff distance parameter, (2) ignoring the local structure of data in computing local densities, (3) using a crisp kernel to calculate local densities, and (4) suffering from the cause of chain reaction. To address these issues, in this paper a new method called DPC-DBFN is proposed. The proposed method uses a fuzzy kernel for improving separability of clusters and reducing the impact of outliers. DPC-DBFN uses a density-based kNN graph for labeling backbones. This strategy prevents the chain reaction and effectively assigns true labels to those instances located on the border regions to effectively cluster data with various shapes and densities. The DPC-DBFN is evaluated on some real-world and synthetic datasets. The experimental results show the effectiveness and robustness of the proposed algorithm.

Access through your organization
Check access to the full text by signing in through your organization.

Introduction
Data clustering is a machine learning task that aims at partitioning a group of objects into a set of subgroups to achieve both high intragroup and low intergroup similarities. In other words, the members of each group are similar, yet dissimilar to members of the other groups. The set of clusters resulting from a cluster analysis can be referred to as data clustering. Data clustering plays an important role in many real-world scenarios and has been studied in many research fields such as machine learning [1], pattern recognition [2], optimization [3],[4], image analysis [5], cybersecurity [6], and bioinformatics [7] to mention a few. In recent years many clustering methods have been proposed from different perspectives. These methods are categorized into four main groups: objective-based [8], model-based [9],[10], hierarchical [11], and density-based [12] methods.

Objective-based methods partition the given dataset into several clusters and generally starts with an initial partition and then use an iterative process to optimize an objective function. Although they are simple and efficient methods, their resulting clustering are susceptible to the presence of noise and non-convex shape of data distribution. In other words, their efficiency will be reduced facing with non-spherical clusters. k-means [13] is an example of this category. Model-based clustering methods suppose that the instances of a cluster are most likely to be derived from a unique probabilistic model. These methods, generally adopt a fixed number of models to approximate the distribution of objects. However, it is usually difficult to know the model or describe the data distribution before performing the clustering process. Expectation Maximization(EM) [14] is a well-known model-based algorithm. Hierarchical methods aims at organizing the instances as a tree. Chameleon [15], single-link, complete-link, and Ward are well-known methods in this category [16]. However, they are computationally expensive specially when they are applied to large datasets. Finally, density-based methods aim at identifying dense regions separated by regions of lower point density. They suppose that data in the sparse regions is mostly considered as noise or border points; thus these methods can identify non-spherical clusters with any arbitrary shapes in the feature space. DBSCAN [17] and OPTICS [18] are two well-known examples of the density-based clustering methods. Most of these methods don't require to make any assumption about the distribution of the data to estimate the density of groups. In addition, they don't need the number of clusters as a user-defined parameter before starting the clustering process. However, they are very sensitive to the values of their adjustable parameters, results in generating different clustering of the data even for slightly changing the values of the parameters. Besides, they could not properly identify the clusters with overlapping densities [19].

To overcome the above mentioned issues, recently density peaks clustering (DPC) [20] is proposed as a fast method for detecting non-spherical clusters without requiring to specify the number of clusters. Unlike the other density-based methods, DPC is considered as a hybridization of centroid and density clustering approaches. The main goal of DPC is to identify those data points having higher densities than their neighbors as cluster centers. In general, these centers should relatively be far enough from each other. This method is not an iterative process, and thus it is faster than the other clustering algorithms. However, the original DPC suffers from several shortcomings. First, it is relatively difficult to identify a reasonable criterion to estimate the optimal value of the DPC parameter. For example, choosing a proper value for the cutoff distance as a density metric can influence the clustering results. Second, its original label propagation strategy can cause propagation of errors. This especially occurs for those datasets that contain highly overlapped clusters with arbitrary shapes. To overcome these limitations, several research efforts are recently proposed in the literature. For example, authors of [21] proposed a method called DPC-KNN by introducing the idea of the k nearest neighbors (kNN) to compute the local density values. This method also employs the principal component analysis (PCA) to reduce the data dimensionality. Although, this idea increase the performance of the method; however, it results in a lower performance on clustering the data contains imbalanced clusters, overlapped clusters, and complex shapes. The authors of [22] proposed a specific local density metric by combining the kNN with fuzzy set theory. This method which is so called FKNN is more robust than the original DPC and it is effective in clustering complex shapes. However, it cannot be applied to identify overlapped clusters.

In this paper, inspired by the idea of DPC-KNN [21] and FKNN [22], we propose an enhanced Density Peaks Clustering method based on Density Backbone and Fuzzy Neighborhood (DPC-DBFN). The proposed method is simple, robust, scalable, and effective which is able to identify clusters with arbitrary shapes, sizes and densities. The main property of the proposed method is identifying cluster backbones instead of cluster centers. To this aim, a density metric based on fuzzy neighborhood relation is proposed for computing the local densities. Compared to the crisp neighborhood relation, using fuzzy relations leads to assigning different membership values to the neighborhoods of a data point. Moreover, the proposed fuzzy kernel is robust enough to perform on complex data with any cluster sizes and shapes. Moreover, a specific method is proposed to identify high-density regions which are known as cluster backbones. The points within a cluster backbone are connected to their k nearest neighbors based on their densities. The remaining data points consist of noises and borders. For each of these points, a specific strategy is used to assign them to their most likely clusters. Hence, DPC-DBFN automatically adapts to the different densities of clusters and does not depends on any user-defined density criterion. Therefore, the proposed method can identify the correct cluster backbones and detect clusters with any densities and any shapes. Several experiments have been performed to assess the performance of the proposed DPC-DBFN method and compare its results with the results obtained from the original DPC method and its state-of-the-art extensions. The experimental results show the superiority of the proposed method. The main novelties and main properties of the proposed method in comparison with state-of-the-art methods are listed as follows:
1-
In the original DPC and most of its improvements [20],[23], the wrong assignment of a label is propagated to the other unassigned points. This is due to the chain reaction in the label assignment strategy. To solve this issue, the proposed method forms the cluster backbones by identifying dense points and by assigning the border points to their nearest cluster backbones. This strategy prevents the chain reactions because the identified backbones are far enough to each other. Thus, the border points that are reachable from more than one cluster can only be placed into one cluster according to their nearest cluster backbones.

2-
Most of DPC extensions such as methods given in [20],[24] employ the cutoff distance parameter of the original DPC to estimate densities. This measure has some difficulties in reliable estimating of densities. The results of these methods are very sensitive to the values of this parameter. It seems that there is no way to find out the best value of the parameter. To this end, in this paper, a fuzzy neighborhood kernel is used to compute the local densities. For easy identification of backbones, this measure aims at reducing the impact of outliers in computing the densities to enhance the separability of the cluster backbones. Therefore, employing this measure results in identifying effective cluster cores as well as improve the robustness and efficiency of the proposed method.

3-
Although both of the proposed method and the DBSCAN are relying on defining representative points, computing local density relationships, and removing the noises, however DBSCAN uses two adjustable parameters which a small change of them makes varying clustering results. While the proposed method just uses a user-defined parameter which it has a lower sensitivity to various values of this parameter. Thus, it can be effectively applied to data with any sizes of clusters.

4-
In the original DPC and most of its improvements, the ``strong'' clusters tend to mask the weaker ones. To overcome this limitation, our method first uses a fuzzy neighborhood measure to rank the points based on their densities and identifies top c dense points as cluster centers. Each center is assigned by a distinct label and then these labels are propagated to the neighbors of the centers. This process is repeated for the dense neighbors of neighbors until all dense points are assigned by labels. This process leads to form all cluster backbones with various shapes and sizes. This is why that the proposed method prevents masking of weaker clusters and thus it is effective enough for clustering the complex data.


The rest of paper is organized as follows. The related work is presented in Section 2. Section 3 describes the problem formulation and proposed method. Experimental results are presented in Section 4 and the discussion is given in Section 5. Finally, the concluding remarks and future work are presented in Section 6.

Section snippets
Density peaks clustering methods
Density peaks clustering (DPC) algorithm [20] is a combination of centroid and density-based clustering methods. This method identifies cluster centers among those nodes that have higher local density values than their neighbors and the centers are relatively far enough to each other. Considering this idea, the cluster centers are recognized intuitively, and then clusters are identified without affecting by the shape of the clusters and the size of the data [20]. Unlike the other clustering

The proposed method
This section aims at presenting details of the proposed clustering method known as Density Peaks Clustering based on Density Backbones and Fuzzy Neighborhood (DPC-DBFN). The proposed method constitutes three main steps: (1) identifying cluster centers, (2) forming cluster backbones, and (3) label assignment. In the first step, a novel fuzzy kernel is proposed to compute the local density values. Unlike the original DPC that uses the crisp neighborhood relation to calculate local density, the

Experiments
Several experiments are performed to evaluate the performance of the proposed method in comparison with some baseline and state-of-the-art methods such as DPC-KNN, IDPC, and FKNN. The experiments are carried out on a computer with an Intel Xeon E5-2630 v4 CPU, 64G RAM, Windows 7 64bit OS, and MATLAB 2016b programming environment. The results are reported based on several performance metrics such as Purity, Normalized Mutual Information (NMI), Rand Index (RI), and the Adjusted Rand Index (ARI).

Discussion and analysis
In this section, we first discuss the shortcomings of the original DPC and some of its improvements which cause them fail to give good clustering results in some cases. For example, for small datasets it is difficult for DPC to make a reliable estimate of the densities. In this case, the clustering results of DPC can be greatly affected by the cutoff distance. The proposed method employs the idea of the fuzzy neighborhood to calculate the local densities with the aim of reducing the influence

Conclusion
This paper proposed an efficient clustering algorithm called DPC-DBFN, which can easily find out clusters with various densities, shapes, and sizes. The proposed method aims at clustering the data in three main steps: identifying cluster centers, forming backbones, and label assignments. The first step utilizes a novel fuzzy kernel in the local density calculation. This kernel computes the local densities in such a way having a high difference between dense points and outliers to improve the

Acknowledgement
We would like to thank the anonymous reviewers for their valuable comments and suggestions which improved the paper. We also thank Mr. Seyed Amjad Seyedi for his help in providing vector-based graphs.